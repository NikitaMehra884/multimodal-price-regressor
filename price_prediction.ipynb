{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5212217c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning function is ready.\n"
     ]
    }
   ],
   "source": [
    "# English ke aam shabd (jaise 'the', 'is', 'a') ki list\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
    "    words = text.split()\n",
    "    clean_words = [word for word in words if word not in stop_words]\n",
    "    text = ' '.join(clean_words)\n",
    "    return text\n",
    "\n",
    "print(\"Cleaning function is ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "840fe3a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text cleaning complete for both train and test data.\n"
     ]
    }
   ],
   "source": [
    "train_df['cleaned_content'] = train_df['catalog_content'].apply(clean_text)\n",
    "test_df['cleaned_content'] = test_df['catalog_content'].apply(clean_text)\n",
    "\n",
    "print(\"Text cleaning complete for both train and test data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "92ecae60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>catalog_content</th>\n",
       "      <th>cleaned_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Item Name: La Victoria Green Taco Sauce Mild, ...</td>\n",
       "      <td>item name la victoria green taco sauce mild 12...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Item Name: Salerno Cookies, The Original Butte...</td>\n",
       "      <td>item name salerno cookies original butter cook...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Item Name: Bear Creek Hearty Soup Bowl, Creamy...</td>\n",
       "      <td>item name bear creek hearty soup bowl creamy c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Item Name: Judee’s Blue Cheese Powder 11.25 oz...</td>\n",
       "      <td>item name judees blue cheese powder 1125 oz gl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Item Name: kedem Sherry Cooking Wine, 12.7 Oun...</td>\n",
       "      <td>item name kedem sherry cooking wine 127 ounce ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     catalog_content  \\\n",
       "0  Item Name: La Victoria Green Taco Sauce Mild, ...   \n",
       "1  Item Name: Salerno Cookies, The Original Butte...   \n",
       "2  Item Name: Bear Creek Hearty Soup Bowl, Creamy...   \n",
       "3  Item Name: Judee’s Blue Cheese Powder 11.25 oz...   \n",
       "4  Item Name: kedem Sherry Cooking Wine, 12.7 Oun...   \n",
       "\n",
       "                                     cleaned_content  \n",
       "0  item name la victoria green taco sauce mild 12...  \n",
       "1  item name salerno cookies original butter cook...  \n",
       "2  item name bear creek hearty soup bowl creamy c...  \n",
       "3  item name judees blue cheese powder 1125 oz gl...  \n",
       "4  item name kedem sherry cooking wine 127 ounce ...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[['catalog_content', 'cleaned_content']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a42846bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF process complete.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# TF-IDF model banayein\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
    "\n",
    "# Training data par fit aur transform karein\n",
    "text_features_train = tfidf_vectorizer.fit_transform(train_df['cleaned_content'])\n",
    "\n",
    "# Test data par sirf transform karein\n",
    "text_features_test = tfidf_vectorizer.transform(test_df['cleaned_content'])\n",
    "\n",
    "print(\"TF-IDF process complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7015e8ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training text features: (75000, 5000)\n",
      "Shape of testing text features: (75000, 5000)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of training text features:\", text_features_train.shape)\n",
    "print(\"Shape of testing text features:\", text_features_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c6aae9c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_id</th>\n",
       "      <th>catalog_content</th>\n",
       "      <th>image_link</th>\n",
       "      <th>price</th>\n",
       "      <th>cleaned_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>33127</td>\n",
       "      <td>Item Name: La Victoria Green Taco Sauce Mild, ...</td>\n",
       "      <td>https://m.media-amazon.com/images/I/51mo8htwTH...</td>\n",
       "      <td>4.89</td>\n",
       "      <td>item name la victoria green taco sauce mild 12...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>198967</td>\n",
       "      <td>Item Name: Salerno Cookies, The Original Butte...</td>\n",
       "      <td>https://m.media-amazon.com/images/I/71YtriIHAA...</td>\n",
       "      <td>13.12</td>\n",
       "      <td>item name salerno cookies original butter cook...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>261251</td>\n",
       "      <td>Item Name: Bear Creek Hearty Soup Bowl, Creamy...</td>\n",
       "      <td>https://m.media-amazon.com/images/I/51+PFEe-w-...</td>\n",
       "      <td>1.97</td>\n",
       "      <td>item name bear creek hearty soup bowl creamy c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>55858</td>\n",
       "      <td>Item Name: Judee’s Blue Cheese Powder 11.25 oz...</td>\n",
       "      <td>https://m.media-amazon.com/images/I/41mu0HAToD...</td>\n",
       "      <td>30.34</td>\n",
       "      <td>item name judees blue cheese powder 1125 oz gl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>292686</td>\n",
       "      <td>Item Name: kedem Sherry Cooking Wine, 12.7 Oun...</td>\n",
       "      <td>https://m.media-amazon.com/images/I/41sA037+Qv...</td>\n",
       "      <td>66.49</td>\n",
       "      <td>item name kedem sherry cooking wine 127 ounce ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sample_id                                    catalog_content  \\\n",
       "0      33127  Item Name: La Victoria Green Taco Sauce Mild, ...   \n",
       "1     198967  Item Name: Salerno Cookies, The Original Butte...   \n",
       "2     261251  Item Name: Bear Creek Hearty Soup Bowl, Creamy...   \n",
       "3      55858  Item Name: Judee’s Blue Cheese Powder 11.25 oz...   \n",
       "4     292686  Item Name: kedem Sherry Cooking Wine, 12.7 Oun...   \n",
       "\n",
       "                                          image_link  price  \\\n",
       "0  https://m.media-amazon.com/images/I/51mo8htwTH...   4.89   \n",
       "1  https://m.media-amazon.com/images/I/71YtriIHAA...  13.12   \n",
       "2  https://m.media-amazon.com/images/I/51+PFEe-w-...   1.97   \n",
       "3  https://m.media-amazon.com/images/I/41mu0HAToD...  30.34   \n",
       "4  https://m.media-amazon.com/images/I/41sA037+Qv...  66.49   \n",
       "\n",
       "                                     cleaned_content  \n",
       "0  item name la victoria green taco sauce mild 12...  \n",
       "1  item name salerno cookies original butter cook...  \n",
       "2  item name bear creek hearty soup bowl creamy c...  \n",
       "3  item name judees blue cheese powder 1125 oz gl...  \n",
       "4  item name kedem sherry cooking wine 127 ounce ...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb9da63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "17cf893c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 150000 images to download with 16 parallel workers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150000/150000 [00:15<00:00, 9708.84it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "High-speed download process poora hua!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "from tqdm import tqdm\n",
    "import concurrent.futures\n",
    "\n",
    "# --- Yahan se Helper Functions Shuru ---\n",
    "\n",
    "def requests_retry_session(retries=3, backoff_factor=0.3, status_forcelist=(500, 502, 504), session=None):\n",
    "    session = session or requests.Session()\n",
    "    retry = Retry(\n",
    "        total=retries, read=retries, connect=retries,\n",
    "        backoff_factor=backoff_factor, status_forcelist=status_forcelist,\n",
    "    )\n",
    "    adapter = HTTPAdapter(max_retries=retry)\n",
    "    session.mount('http://', adapter)\n",
    "    session.mount('https://', adapter)\n",
    "    return session\n",
    "\n",
    "# Yeh function sirf ek image ko download karega\n",
    "def download_one_image(args):\n",
    "    url, image_path, session = args\n",
    "    if not os.path.exists(image_path):\n",
    "        try:\n",
    "            response = session.get(url, timeout=30)\n",
    "            if response.status_code == 200:\n",
    "                with open(image_path, 'wb') as f:\n",
    "                    f.write(response.content)\n",
    "                return \"Downloaded\"\n",
    "            else:\n",
    "                return f\"Failed with status {response.status_code}\"\n",
    "        except Exception as e:\n",
    "            return f\"Failed with error: {e}\"\n",
    "    else:\n",
    "        return \"Already Exists\"\n",
    "\n",
    "# --- Yahan se Main Code Shuru ---\n",
    "\n",
    "# Ek saath kitne downloads karne hain. 16 se shuru karein.\n",
    "# Agar aapka internet bahut accha hai to ise 32 kar sakte hain.\n",
    "MAX_WORKERS = 16\n",
    "\n",
    "# images folder banayein\n",
    "os.makedirs('../images', exist_ok=True)\n",
    "\n",
    "# Dono dataframes ko jod dein\n",
    "all_df = pd.concat([train_df, test_df], ignore_index=True)\n",
    "\n",
    "# Download ke liye tasks ki list banayein\n",
    "tasks = []\n",
    "session = requests_retry_session()\n",
    "for index, row in all_df.iterrows():\n",
    "    url = row['image_link']\n",
    "    image_id = row['sample_id']\n",
    "    image_path = os.path.join('../images', f\"{image_id}.jpg\")\n",
    "    tasks.append((url, image_path, session))\n",
    "\n",
    "print(f\"Total {len(tasks)} images to download with {MAX_WORKERS} parallel workers...\")\n",
    "\n",
    "# ThreadPoolExecutor ka istemal karke parallel download karein\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "    # tqdm progress bar ke saath sabhi tasks ko run karein\n",
    "    results = list(tqdm(executor.map(download_one_image, tasks), total=len(tasks)))\n",
    "\n",
    "print(\"\\nHigh-speed download process poora hua!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4c76dd6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet50 model successfully loaded.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.models import Model\n",
    "from tqdm import tqdm\n",
    "import concurrent.futures\n",
    "\n",
    "# ResNet50 model ko 'imagenet' ke pre-trained weights ke saath load karein\n",
    "# include_top=False ka matlab hai ki humein aakhri classification layer nahi chahiye\n",
    "# pooling='avg' se humein ek flat feature vector (1D array) milta hai\n",
    "base_model = ResNet50(weights='imagenet', include_top=False, pooling='avg')\n",
    "\n",
    "print(\"ResNet50 model successfully loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4eac39ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image feature extraction function is ready.\n"
     ]
    }
   ],
   "source": [
    "def extract_image_features(img_path):\n",
    "    try:\n",
    "        # Image ko load karein aur 224x224 size mein resize karein\n",
    "        img = image.load_img(img_path, target_size=(224, 224))\n",
    "        \n",
    "        # Image ko array mein badlein\n",
    "        x = image.img_to_array(img)\n",
    "        \n",
    "        # Ek extra dimension add karein\n",
    "        x = np.expand_dims(x, axis=0)\n",
    "        \n",
    "        # Image ko ResNet50 ke hisaab se preprocess karein\n",
    "        x = preprocess_input(x)\n",
    "        \n",
    "        # Model se features predict karein\n",
    "        features = base_model.predict(x, verbose=0)\n",
    "        \n",
    "        return features.flatten()\n",
    "    except Exception as e:\n",
    "        # Agar image kharab hai ya exist nahi karti, to 2048 zeros ka vector return karein\n",
    "        return np.zeros(2048)\n",
    "\n",
    "print(\"Image feature extraction function is ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b9512e72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features from 75000 training images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 212/75000 [00:24<2:24:19,  8.64it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Training images ke liye\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS_FEATURES) \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m     train_image_features_list = \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexecutor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mextract_image_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_image_paths\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_image_paths\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mExtracting features from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(test_image_paths)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m test images...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Test images ke liye\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nikit\\Desktop\\ML_Challenge\\venv\\Lib\\site-packages\\tqdm\\std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\concurrent\\futures\\_base.py:619\u001b[39m, in \u001b[36mExecutor.map.<locals>.result_iterator\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    616\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m fs:\n\u001b[32m    617\u001b[39m     \u001b[38;5;66;03m# Careful not to keep a reference to the popped future\u001b[39;00m\n\u001b[32m    618\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m619\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[43m_result_or_cancel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    620\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    621\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m _result_or_cancel(fs.pop(), end_time - time.monotonic())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\concurrent\\futures\\_base.py:317\u001b[39m, in \u001b[36m_result_or_cancel\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m    315\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    316\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m317\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfut\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    318\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    319\u001b[39m         fut.cancel()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\concurrent\\futures\\_base.py:451\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m    449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.__get_result()\n\u001b[32m--> \u001b[39m\u001b[32m451\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_condition\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[32m    454\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\threading.py:327\u001b[39m, in \u001b[36mCondition.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    325\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[32m    326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m327\u001b[39m         \u001b[43mwaiter\u001b[49m\u001b[43m.\u001b[49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    328\u001b[39m         gotit = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    329\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Train aur test data ke liye image paths ki list banayein\n",
    "train_image_paths = ['../images/' + str(sid) + '.jpg' for sid in train_df['sample_id']]\n",
    "test_image_paths = ['../images/' + str(sid) + '.jpg' for sid in test_df['sample_id']]\n",
    "\n",
    "MAX_WORKERS_FEATURES = 4 # Isko apne computer ke CPU cores ke hisaab se set karein (4 se 8 accha rehta hai)\n",
    "\n",
    "print(f\"Extracting features from {len(train_image_paths)} training images...\")\n",
    "# Training images ke liye\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS_FEATURES) as executor:\n",
    "    train_image_features_list = list(tqdm(executor.map(extract_image_features, train_image_paths), total=len(train_image_paths)))\n",
    "\n",
    "print(f\"\\nExtracting features from {len(test_image_paths)} test images...\")\n",
    "# Test images ke liye\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS_FEATURES) as executor:\n",
    "    test_image_features_list = list(tqdm(executor.map(extract_image_features, test_image_paths), total=len(test_image_paths)))\n",
    "\n",
    "# Lists ko numpy arrays mein convert karein\n",
    "X_img_train = np.array(train_image_features_list)\n",
    "X_img_test = np.array(test_image_features_list)\n",
    "\n",
    "print(\"\\nFeature extraction for all images complete.\")\n",
    "print(\"Shape of training image features:\", X_img_train.shape)\n",
    "print(\"Shape of testing image features:\", X_img_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "47b908d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting feature extraction for 75000 training images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 354/75000 [00:42<2:29:49,  8.30it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# Extract features for training images in parallel\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS_FEATURES) \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m     train_image_features_list = \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexecutor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mextract_image_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_image_paths\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_image_paths\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m X_img_train = np.array(train_image_features_list)\n\u001b[32m     20\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mStarting feature extraction for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(test_image_paths)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m test images...\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nikit\\Desktop\\ML_Challenge\\venv\\Lib\\site-packages\\tqdm\\std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\concurrent\\futures\\_base.py:619\u001b[39m, in \u001b[36mExecutor.map.<locals>.result_iterator\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    616\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m fs:\n\u001b[32m    617\u001b[39m     \u001b[38;5;66;03m# Careful not to keep a reference to the popped future\u001b[39;00m\n\u001b[32m    618\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m619\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[43m_result_or_cancel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    620\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    621\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m _result_or_cancel(fs.pop(), end_time - time.monotonic())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\concurrent\\futures\\_base.py:317\u001b[39m, in \u001b[36m_result_or_cancel\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m    315\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    316\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m317\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfut\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    318\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    319\u001b[39m         fut.cancel()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\concurrent\\futures\\_base.py:451\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m    449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.__get_result()\n\u001b[32m--> \u001b[39m\u001b[32m451\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_condition\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[32m    454\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\threading.py:327\u001b[39m, in \u001b[36mCondition.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    325\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[32m    326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m327\u001b[39m         \u001b[43mwaiter\u001b[49m\u001b[43m.\u001b[49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    328\u001b[39m         gotit = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    329\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import concurrent.futures # This was the missing import line\n",
    "\n",
    "# Make sure the 'extract_image_features' function has been defined in a previous cell.\n",
    "\n",
    "# Create lists of image paths for both train and test data\n",
    "train_image_paths = ['../images/' + str(sid) + '.jpg' for sid in train_df['sample_id']]\n",
    "test_image_paths = ['../images/' + str(sid) + '.jpg' for sid in test_df['sample_id']]\n",
    "\n",
    "# Set the number of workers based on your CPU\n",
    "MAX_WORKERS_FEATURES = 4 \n",
    "\n",
    "print(f\"Starting feature extraction for {len(train_image_paths)} training images...\")\n",
    "# Extract features for training images in parallel\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS_FEATURES) as executor:\n",
    "    train_image_features_list = list(tqdm(executor.map(extract_image_features, train_image_paths), total=len(train_image_paths)))\n",
    "X_img_train = np.array(train_image_features_list)\n",
    "\n",
    "print(f\"\\nStarting feature extraction for {len(test_image_paths)} test images...\")\n",
    "# Extract features for test images in parallel\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS_FEATURES) as executor:\n",
    "    test_image_features_list = list(tqdm(executor.map(extract_image_features, test_image_paths), total=len(test_image_paths)))\n",
    "X_img_test = np.array(test_image_features_list)\n",
    "\n",
    "# Save the results to .npy files\n",
    "np.save('X_img_train.npy', X_img_train)\n",
    "np.save('X_img_test.npy', X_img_test)\n",
    "\n",
    "print(\"\\nFeature extraction is complete AND the results have been saved to .npy files!\")\n",
    "print(\"Shape of training image features:\", X_img_train.shape)\n",
    "print(\"Shape of testing image features:\", X_img_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "69fb25bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Text Cleaning Shuru ---\n",
      "Text cleaning poora hua. 'cleaned_content' column ban gaya hai.\n",
      "Sample:\n",
      "                                     catalog_content  \\\n",
      "0  Item Name: La Victoria Green Taco Sauce Mild, ...   \n",
      "1  Item Name: Salerno Cookies, The Original Butte...   \n",
      "2  Item Name: Bear Creek Hearty Soup Bowl, Creamy...   \n",
      "3  Item Name: Judee’s Blue Cheese Powder 11.25 oz...   \n",
      "4  Item Name: kedem Sherry Cooking Wine, 12.7 Oun...   \n",
      "\n",
      "                                     cleaned_content  \n",
      "0  item name la victoria green taco sauce mild 12...  \n",
      "1  item name salerno cookies original butter cook...  \n",
      "2  item name bear creek hearty soup bowl creamy c...  \n",
      "3  item name judees blue cheese powder 1125 oz gl...  \n",
      "4  item name kedem sherry cooking wine 127 ounce ...  \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "print(\"--- Text Cleaning Shuru ---\")\n",
    "# nltk.download('stopwords') # Agar pehle se nahi kiya hai to is line ko uncomment karke chalayein\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
    "    words = text.split()\n",
    "    clean_words = [word for word in words if word not in stop_words]\n",
    "    return ' '.join(clean_words)\n",
    "\n",
    "# 'cleaned_content' column banayein\n",
    "train_df['cleaned_content'] = train_df['catalog_content'].apply(clean_text)\n",
    "test_df['cleaned_content'] = test_df['catalog_content'].apply(clean_text)\n",
    "\n",
    "print(\"Text cleaning poora hua. 'cleaned_content' column ban gaya hai.\")\n",
    "print(\"Sample:\")\n",
    "print(train_df[['catalog_content', 'cleaned_content']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2e27ed9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Features ko Jodna ---\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'X_img_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Note: Is cell ko chalane se pehle, text_features_train naam ka variable\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# (jo TF-IDF se bana tha) aapki notebook ki memory mein hona chahiye.\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m--- Features ko Jodna ---\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m X_train_final = hstack([text_features_train, \u001b[43mX_img_train\u001b[49m])\n\u001b[32m      8\u001b[39m X_test_final = hstack([text_features_test, X_img_test])\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mCombined features taiyaar hain.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'X_img_train' is not defined"
     ]
    }
   ],
   "source": [
    "from scipy.sparse import hstack\n",
    "\n",
    "# Note: Is cell ko chalane se pehle, text_features_train naam ka variable\n",
    "# (jo TF-IDF se bana tha) aapki notebook ki memory mein hona chahiye.\n",
    "\n",
    "print(\"--- Features ko Jodna ---\")\n",
    "X_train_final = hstack([text_features_train, X_img_train])\n",
    "X_test_final = hstack([text_features_test, X_img_test])\n",
    "\n",
    "print(\"Combined features taiyaar hain.\")\n",
    "print(\"Final training features ka shape:\", X_train_final.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e22b70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57aef128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Model Training ki Taiyari ---\n",
      "Data training aur validation ke liye ready hai.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "print(\"--- Model Training ki Taiyari ---\")\n",
    "# Target variable (price) ko alag karein\n",
    "y = train_df['price']\n",
    "# Log transform lagayein taaki skewed data handle ho sake\n",
    "y_log = np.log1p(y)\n",
    "\n",
    "# Data ko 80% train aur 20% validation set mein baantein\n",
    "X_train_split, X_val, y_train_split, y_val = train_test_split(\n",
    "    X_train_final, y_log, \n",
    "    test_size=0.2, \n",
    "    random_state=42\n",
    ")\n",
    "print(\"Data training aur validation ke liye ready hai.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5562eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Model ko Train Karna ---\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 1.122327 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 536934\n",
      "[LightGBM] [Info] Number of data points in the train set: 60000, number of used features: 4991\n",
      "[LightGBM] [Info] Start training from score 2.740904\n",
      "Model training complete.\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "print(\"--- Model ko Train Karna ---\")\n",
    "model = lgb.LGBMRegressor(random_state=42)\n",
    "# Model ko training data par train karein\n",
    "model.fit(X_train_split, y_train_split)\n",
    "\n",
    "print(\"Model training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f4bf13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Model ki Performance Check Karna (SMAPE) ---\n",
      "Validation SMAPE Score: 57.5732% (Jitna kam, utna behtar)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nikit\\Desktop\\ML_Challenge\\venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# SMAPE score calculate karne ke liye function\n",
    "def smape(y_true, y_pred):\n",
    "    # Yeh formula challenge ke document se liya gaya hai\n",
    "    numerator = np.abs(y_pred - y_true)\n",
    "    denominator = (np.abs(y_true) + np.abs(y_pred)) / 2\n",
    "    return np.mean(numerator / denominator) * 100\n",
    "\n",
    "print(\"--- Model ki Performance Check Karna (SMAPE) ---\")\n",
    "# Validation set par price predict karein\n",
    "val_preds_log = model.predict(X_val)\n",
    "# Log transform ko reverse karein\n",
    "val_preds = np.expm1(val_preds_log)\n",
    "y_val_orig = np.expm1(y_val)\n",
    "\n",
    "# SMAPE score calculate karein\n",
    "smape_score = smape(y_val_orig, val_preds)\n",
    "print(f\"Validation SMAPE Score: {smape_score:.4f}% (Jitna kam, utna behtar)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b1c4d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Final Model ko Poore Data par Re-train Karna ---\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.984523 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 605627\n",
      "[LightGBM] [Info] Number of data points in the train set: 75000, number of used features: 4996\n",
      "[LightGBM] [Info] Start training from score 2.739217\n",
      "Final model re-training complete.\n",
      "\n",
      "--- Submission File Banana ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nikit\\Desktop\\ML_Challenge\\venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\nikit\\Desktop\\ML_Challenge\\venv\\Lib\\site-packages\\lightgbm\\basic.py:1238: UserWarning: Converting data to scipy sparse matrix.\n",
      "  _log_warning(\"Converting data to scipy sparse matrix.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Submission file 'test_out.csv' taiyaar hai!\n",
      "File ka sample:\n",
      "   sample_id      price\n",
      "0     100179  17.106771\n",
      "1     245611  17.484260\n",
      "2     146263  28.837859\n",
      "3      95658  11.714338\n",
      "4      36806  19.668761\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Final Model ko Poore Data par Re-train Karna ---\")\n",
    "model.fit(X_train_final, y_log)\n",
    "print(\"Final model re-training complete.\")\n",
    "\n",
    "print(\"\\n--- Submission File Banana ---\")\n",
    "# Final test features par predict karein\n",
    "test_preds_log = model.predict(X_test_final)\n",
    "# Log transform ko reverse karein\n",
    "final_prices = np.expm1(test_preds_log)\n",
    "# Sunishchit karein ki koi bhi price negative na ho\n",
    "final_prices[final_prices < 0] = 0\n",
    "\n",
    "# Submission ke liye DataFrame banayein\n",
    "submission_df = pd.DataFrame({\n",
    "    'sample_id': test_df['sample_id'],\n",
    "    'price': final_prices\n",
    "})\n",
    "# CSV file save karein\n",
    "submission_df.to_csv('test_out.csv', index=False)\n",
    "\n",
    "print(\"\\nSubmission file 'test_out.csv' taiyaar hai!\")\n",
    "print(\"File ka sample:\")\n",
    "print(submission_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96f06d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Step 1: Text Cleaning Shuru ---\n",
      "Text cleaning poora hua. 'cleaned_content' column ban gaya hai.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "print(\"--- Step 1: Text Cleaning Shuru ---\")\n",
    "# nltk.download('stopwords') # Agar pehle se nahi kiya hai to is line ko uncomment karke chalayein\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
    "    words = text.split()\n",
    "    clean_words = [word for word in words if word not in stop_words]\n",
    "    return ' '.join(clean_words)\n",
    "\n",
    "train_df['cleaned_content'] = train_df['catalog_content'].apply(clean_text)\n",
    "test_df['cleaned_content'] = test_df['catalog_content'].apply(clean_text)\n",
    "print(\"Text cleaning poora hua. 'cleaned_content' column ban gaya hai.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6c48aee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d62ac0b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Apne dataset ka sahi path dein\n",
    "train_df = pd.read_csv('../dataset/train.csv')\n",
    "test_df = pd.read_csv('../dataset/test.csv')\n",
    "\n",
    "print(\"Data loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b66b4cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\nikit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c71c3ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading DistilBERT model (yeh pehli baar mein thoda time lega)...\n",
      "\n",
      "Starting feature extraction with DistilBERT in batches of 32...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4688/4688 [3:33:27<00:00,  2.73s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DistilBERT features banakar 'distilbert_embeddings.npy' file mein save ho gaye hain!\n",
      "Embeddings ka shape: (150000, 768)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- Model aur Tokenizer Load Karein ---\n",
    "print(\"Loading DistilBERT model (yeh pehli baar mein thoda time lega)...\")\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# --- Data Taiyaar Karein ---\n",
    "# train_df aur test_df pehle se loaded hone chahiye\n",
    "all_text = pd.concat([train_df, test_df], ignore_index=True)['cleaned_content'].tolist()\n",
    "all_embeddings = []\n",
    "\n",
    "# --- Batch Processing Shuru Karein ---\n",
    "# Batch size aapki RAM par depend karta hai. 32 ek safe value hai.\n",
    "# Agar aapke paas 16GB ya zyada RAM hai to ise 64 kar sakte hain.\n",
    "batch_size = 32 \n",
    "\n",
    "print(f\"\\nStarting feature extraction with DistilBERT in batches of {batch_size}...\")\n",
    "\n",
    "for i in tqdm(range(0, len(all_text), batch_size)):\n",
    "    batch = all_text[i:i + batch_size]\n",
    "    \n",
    "    # Text ko model ke format mein badlein\n",
    "    inputs = tokenizer(batch, return_tensors='pt', truncation=True, padding=True, max_length=128)\n",
    "    \n",
    "    # Memory bachane ke liye torch.no_grad() ka istemal karein\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        \n",
    "    # Sentence ka embedding nikalne ke liye mean ka istemal karein\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1).cpu().numpy()\n",
    "    all_embeddings.append(embeddings)\n",
    "\n",
    "# Sabhi batches ke embeddings ko ek saath jodein\n",
    "final_embeddings = np.vstack(all_embeddings)\n",
    "\n",
    "# In features ko ek file mein save kar lein, taaki dobara calculate na karna pade\n",
    "np.save('distilbert_embeddings.npy', final_embeddings)\n",
    "\n",
    "print(\"\\nDistilBERT features banakar 'distilbert_embeddings.npy' file mein save ho gaye hain!\")\n",
    "print(\"Embeddings ka shape:\", final_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c7138e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Step 1: Naye DistilBERT aur Image Features ko Jodna ---\n",
      "Naye combined features taiyaar hain. Shape: (75000, 2816)\n",
      "\n",
      "--- Step 2: Model Training ki Taiyari ---\n",
      "Data training aur validation ke liye ready hai.\n",
      "\n",
      "--- Step 3: Model ko Naye Features par Train Karna ---\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.796956 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 195840\n",
      "[LightGBM] [Info] Number of data points in the train set: 60000, number of used features: 768\n",
      "[LightGBM] [Info] Start training from score 2.740904\n",
      "Model training complete.\n",
      "\n",
      "--- Step 4: Nayi Performance Check Karna (SMAPE) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nikit\\Desktop\\ML_Challenge\\venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naye Features ke saath Validation SMAPE Score: 63.5097%\n",
      "\n",
      "--- Step 5: Final Model ko Poore Data par Re-train Karna ---\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.830974 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 195840\n",
      "[LightGBM] [Info] Number of data points in the train set: 75000, number of used features: 768\n",
      "[LightGBM] [Info] Start training from score 2.739217\n",
      "Final model re-training complete.\n",
      "\n",
      "--- Step 6: Submission File Banana ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nikit\\Desktop\\ML_Challenge\\venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nayi submission file 'test_out_bert_features.csv' taiyaar hai!\n",
      "   sample_id      price\n",
      "0     100179  17.669095\n",
      "1     245611  17.244876\n",
      "2     146263  21.181437\n",
      "3      95658  16.719877\n",
      "4      36806  22.975845\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.model_selection import train_test_split\n",
    "import lightgbm as lgb\n",
    "\n",
    "# --- Step 1: Naye Features Load aur Combine Karein ---\n",
    "print(\"--- Step 1: Naye DistilBERT aur Image Features ko Jodna ---\")\n",
    "# Save kiye gaye DistilBERT features ko load karein\n",
    "distilbert_features = np.load('distilbert_embeddings.npy')\n",
    "\n",
    "# Training aur Test features ko alag karein\n",
    "bert_features_train = distilbert_features[:len(train_df)]\n",
    "bert_features_test = distilbert_features[len(train_df):]\n",
    "\n",
    "# Naye text features ko image features ke saath jodein (X_img_train pehle se bana hona chahiye)\n",
    "X_train_final_new = np.concatenate([bert_features_train, X_img_train], axis=1)\n",
    "X_test_final_new = np.concatenate([bert_features_test, X_img_test], axis=1)\n",
    "print(\"Naye combined features taiyaar hain. Shape:\", X_train_final_new.shape)\n",
    "\n",
    "\n",
    "# --- BAAKI KA PROCESS BILKUL SAME RAHEGA ---\n",
    "\n",
    "# --- Step 2: Model Training ki Taiyari ---\n",
    "print(\"\\n--- Step 2: Model Training ki Taiyari ---\")\n",
    "y = train_df['price']\n",
    "y_log = np.log1p(y)\n",
    "X_train_split, X_val, y_train_split, y_val = train_test_split(\n",
    "    X_train_final_new, y_log, test_size=0.2, random_state=42\n",
    ")\n",
    "print(\"Data training aur validation ke liye ready hai.\")\n",
    "\n",
    "# --- Step 3: Model ko Naye Features par Train Karna ---\n",
    "print(\"\\n--- Step 3: Model ko Naye Features par Train Karna ---\")\n",
    "model_new = lgb.LGBMRegressor(random_state=42)\n",
    "model_new.fit(X_train_split, y_train_split)\n",
    "print(\"Model training complete.\")\n",
    "\n",
    "# --- Step 4: Nayi Performance Check Karna (SMAPE) ---\n",
    "print(\"\\n--- Step 4: Nayi Performance Check Karna (SMAPE) ---\")\n",
    "def smape(y_true, y_pred):\n",
    "    numerator = np.abs(y_pred - y_true)\n",
    "    denominator = (np.abs(y_true) + np.abs(y_pred)) / 2\n",
    "    return np.mean(numerator / denominator) * 100\n",
    "\n",
    "val_preds_log = model_new.predict(X_val)\n",
    "val_preds = np.expm1(val_preds_log)\n",
    "y_val_orig = np.expm1(y_val)\n",
    "smape_score = smape(y_val_orig, val_preds)\n",
    "print(f\"Naye Features ke saath Validation SMAPE Score: {smape_score:.4f}%\")\n",
    "\n",
    "# --- Step 5: Final Submission File Banana ---\n",
    "print(\"\\n--- Step 5: Final Model ko Poore Data par Re-train Karna ---\")\n",
    "model_new.fit(X_train_final_new, y_log)\n",
    "print(\"Final model re-training complete.\")\n",
    "\n",
    "print(\"\\n--- Step 6: Submission File Banana ---\")\n",
    "test_preds_log = model_new.predict(X_test_final_new)\n",
    "final_prices = np.expm1(test_preds_log)\n",
    "final_prices[final_prices < 0] = 0\n",
    "\n",
    "submission_df = pd.DataFrame({\n",
    "    'sample_id': test_df['sample_id'],\n",
    "    'price': final_prices\n",
    "})\n",
    "submission_df.to_csv('test_out_bert_features.csv', index=False)\n",
    "print(\"Nayi submission file 'test_out_bert_features.csv' taiyaar hai!\")\n",
    "print(submission_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02edd8bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best features taiyaar hain. Shape: (75000, 7048)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "# Apne pehle se banaye hue feature files ko load karein\n",
    "# (Agar memory mein nahi hain)\n",
    "# text_features_train = ... (TF-IDF wale features)\n",
    "# X_img_train = np.load('X_img_train.npy')\n",
    "\n",
    "# Features ko combine karein\n",
    "X_train_final = hstack([text_features_train, X_img_train])\n",
    "X_test_final = hstack([text_features_test, X_img_test])\n",
    "\n",
    "# Target variable (y_log) taiyaar karein\n",
    "y_log = np.log1p(train_df['price'])\n",
    "\n",
    "print(\"Best features taiyaar hain. Shape:\", X_train_final.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f85bf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LightGBM model...\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 1.217794 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 605627\n",
      "[LightGBM] [Info] Number of data points in the train set: 75000, number of used features: 4996\n",
      "[LightGBM] [Info] Start training from score 2.739217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nikit\\Desktop\\ML_Challenge\\venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\nikit\\Desktop\\ML_Challenge\\venv\\Lib\\site-packages\\lightgbm\\basic.py:1238: UserWarning: Converting data to scipy sparse matrix.\n",
      "  _log_warning(\"Converting data to scipy sparse matrix.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Ridge model...\n",
      "Dono models train ho chuke hain!\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.linear_model import Ridge\n",
    "import numpy as np\n",
    "\n",
    "# --- Model 1: LightGBM (Aapka powerful model) ---\n",
    "print(\"Training LightGBM model...\")\n",
    "# Yahan aap apne tuned parameters bhi daal sakti hain agar aapke paas hain\n",
    "lgbm_model = lgb.LGBMRegressor(random_state=42)\n",
    "lgbm_model.fit(X_train_final, y_log)\n",
    "# Test set par predict karein\n",
    "lgbm_preds_log = lgbm_model.predict(X_test_final)\n",
    "\n",
    "\n",
    "# --- Model 2: Ridge (Ek simple aur fast model) ---\n",
    "print(\"Training Ridge model...\")\n",
    "# Alpha ek regularization parameter hai, 1.0 ek acchi default value hai\n",
    "ridge_model = Ridge(alpha=1.0, random_state=42)\n",
    "ridge_model.fit(X_train_final, y_log)\n",
    "# Test set par predict karein\n",
    "ridge_preds_log = ridge_model.predict(X_test_final)\n",
    "\n",
    "print(\"Dono models train ho chuke hain!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765b7037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble predictions taiyaar hain!\n"
     ]
    }
   ],
   "source": [
    "# Dono models ki predictions ko original scale par wapis laayein\n",
    "lgbm_prices = np.expm1(lgbm_preds_log)\n",
    "ridge_prices = np.expm1(ridge_preds_log)\n",
    "\n",
    "# Weighted Average lein\n",
    "final_prices = (lgbm_prices * 0.75) + (ridge_prices * 0.25)\n",
    "\n",
    "# Sunishchit karein ki koi price negative na ho\n",
    "final_prices[final_prices < 0] = 0\n",
    "\n",
    "print(\"Ensemble predictions taiyaar hain!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83db4bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aapki final submission file 'test_out_ensemble_final.csv' taiyaar hai!\n",
      "Jaldi se ise upload karein!\n",
      "   sample_id      price\n",
      "0     100179  17.169738\n",
      "1     245611  16.259391\n",
      "2     146263  27.383155\n",
      "3      95658  11.627108\n",
      "4      36806  22.903735\n"
     ]
    }
   ],
   "source": [
    "# Submission DataFrame banayein\n",
    "submission_df = pd.DataFrame({\n",
    "    'sample_id': test_df['sample_id'],\n",
    "    'price': final_prices\n",
    "})\n",
    "\n",
    "# CSV file save karein\n",
    "submission_df.to_csv('test_out_ensemble_final.csv', index=False)\n",
    "\n",
    "print(\"Aapki final submission file 'test_out_ensemble_final.csv' taiyaar hai!\")\n",
    "print(\"Jaldi se ise upload karein!\")\n",
    "print(submission_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c4ace5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LightGBM model with tuned parameters...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nikit\\Desktop\\ML_Challenge\\venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\nikit\\Desktop\\ML_Challenge\\venv\\Lib\\site-packages\\lightgbm\\basic.py:1238: UserWarning: Converting data to scipy sparse matrix.\n",
      "  _log_warning(\"Converting data to scipy sparse matrix.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training XGBoost model...\n",
      "\n",
      "Combining predictions from both models...\n",
      "\n",
      "Aapki ultimate ensemble submission file 'test_out_lgbm_xgb_final.csv' taiyaar hai!\n",
      "Ise turant submit karein!\n",
      "   sample_id      price\n",
      "0     100179  15.050247\n",
      "1     245611  17.949546\n",
      "2     146263  22.279576\n",
      "3      95658  12.049100\n",
      "4      36806  24.519219\n"
     ]
    }
   ],
   "source": [
    "# Step 1: XGBoost install karein (agar nahi hai to)\n",
    "\n",
    "\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Note: Isse pehle X_train_final, X_test_final, aur y_log aapki memory mein hone chahiye\n",
    "\n",
    "# --- Model 1: LightGBM ---\n",
    "print(\"Training LightGBM model with tuned parameters...\")\n",
    "# Yeh kuch aam taur par accha perform karne wale parameters hain\n",
    "lgbm_params = {\n",
    "    'objective': 'regression_l1',\n",
    "    'metric': 'rmse',\n",
    "    'n_estimators': 1000,\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.8,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 1,\n",
    "    'lambda_l1': 0.1,\n",
    "    'lambda_l2': 0.1,\n",
    "    'num_leaves': 31,\n",
    "    'verbose': -1,\n",
    "    'n_jobs': -1,\n",
    "    'seed': 42,\n",
    "    'boosting_type': 'gbdt',\n",
    "}\n",
    "\n",
    "lgbm_model = lgb.LGBMRegressor(**lgbm_params)\n",
    "lgbm_model.fit(X_train_final, y_log)\n",
    "lgbm_preds_log = lgbm_model.predict(X_test_final)\n",
    "lgbm_prices = np.expm1(lgbm_preds_log)\n",
    "\n",
    "\n",
    "# --- Model 2: XGBoost ---\n",
    "print(\"\\nTraining XGBoost model...\")\n",
    "xgb_model = xgb.XGBRegressor(\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=7,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    tree_method='hist' # For faster training\n",
    ")\n",
    "xgb_model.fit(X_train_final, y_log)\n",
    "xgb_preds_log = xgb_model.predict(X_test_final)\n",
    "xgb_prices = np.expm1(xgb_preds_log)\n",
    "\n",
    "# --- Step 3: Combine Predictions ---\n",
    "print(\"\\nCombining predictions from both models...\")\n",
    "# Dono powerful models hain, isliye 50-50% average lein\n",
    "final_prices = (lgbm_prices * 0.5) + (xgb_prices * 0.5)\n",
    "final_prices[final_prices < 0] = 0\n",
    "\n",
    "# --- Step 4: Final Submission File ---\n",
    "submission_df = pd.DataFrame({\n",
    "    'sample_id': test_df['sample_id'],\n",
    "    'price': final_prices\n",
    "})\n",
    "submission_df.to_csv('test_out_lgbm_xgb_final.csv', index=False)\n",
    "\n",
    "print(\"\\nAapki ultimate ensemble submission file 'test_out_lgbm_xgb_final.csv' taiyaar hai!\")\n",
    "print(\"Ise turant submit karein!\")\n",
    "print(submission_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8652855e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 5-Fold Cross-Validation Starting ---\n",
      "\n",
      "===== FOLD 1 ===== \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nikit\\Desktop\\ML_Challenge\\venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 SMAPE Score: 57.5732%\n",
      "\n",
      "===== FOLD 2 ===== \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nikit\\Desktop\\ML_Challenge\\venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 SMAPE Score: 56.5241%\n",
      "\n",
      "===== FOLD 3 ===== \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nikit\\Desktop\\ML_Challenge\\venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 SMAPE Score: 56.9172%\n",
      "\n",
      "===== FOLD 4 ===== \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nikit\\Desktop\\ML_Challenge\\venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 SMAPE Score: 55.9047%\n",
      "\n",
      "===== FOLD 5 ===== \n",
      "Fold 5 SMAPE Score: 56.9815%\n",
      "\n",
      "\n",
      "--- Final Result ---\n",
      "Average CV SMAPE Score: 56.7801%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nikit\\Desktop\\ML_Challenge\\venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "\n",
    "# Make sure these variables are already in memory:\n",
    "# X_train_final, y_log, and your smape function\n",
    "\n",
    "# --- THE FIX: Convert the matrix to CSR format for efficient slicing ---\n",
    "X_train_final_csr = X_train_final.tocsr()\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "oof_smape_scores = []\n",
    "\n",
    "print(\"--- 5-Fold Cross-Validation Starting ---\")\n",
    "\n",
    "# Loop through each fold\n",
    "for fold, (train_index, val_index) in enumerate(kf.split(X_train_final_csr, y_log)):\n",
    "    print(f\"\\n===== FOLD {fold+1} ===== \")\n",
    "    \n",
    "    # Slice the data for the current fold using the CSR matrix\n",
    "    X_train_fold, X_val_fold = X_train_final_csr[train_index], X_train_final_csr[val_index]\n",
    "    y_train_fold, y_val_fold = y_log.iloc[train_index], y_log.iloc[val_index]\n",
    "    \n",
    "    # Train the model\n",
    "    model = lgb.LGBMRegressor(random_state=42)\n",
    "    model.fit(X_train_fold, y_train_fold)\n",
    "    \n",
    "    # Calculate the SMAPE score for the fold\n",
    "    val_preds_log = model.predict(X_val_fold)\n",
    "    val_preds = np.expm1(val_preds_log)\n",
    "    y_val_orig = np.expm1(y_val_fold)\n",
    "    \n",
    "    score = smape(y_val_orig, val_preds)\n",
    "    oof_smape_scores.append(score)\n",
    "    print(f\"Fold {fold+1} SMAPE Score: {score:.4f}%\")\n",
    "\n",
    "# Print the average score across all folds\n",
    "print(f\"\\n\\n--- Final Result ---\")\n",
    "print(f\"Average CV SMAPE Score: {np.mean(oof_smape_scores):.4f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a35607",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'distilbert_embeddings.npy' file mil gayi. Features ko direct load kiya ja raha hai...\n",
      "DistilBERT features successfully load ho gaye hain!\n",
      "Final embeddings ka shape: (150000, 768)\n",
      "Naye (DistilBERT + Image) features taiyaar hain!\n",
      "Naye features ka shape: (75000, 2816)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# File ka naam jismein humne features save kiye the\n",
    "embedding_file = 'distilbert_embeddings.npy'\n",
    "\n",
    "if os.path.exists(embedding_file):\n",
    "    print(f\"'{embedding_file}' file mil gayi. Features ko direct load kiya ja raha hai...\")\n",
    "    final_embeddings = np.load(embedding_file)\n",
    "    print(\"DistilBERT features successfully load ho gaye hain!\")\n",
    "else:\n",
    "    # Agar file nahi hai, to use banane ke liye lamba process chalana padega\n",
    "    print(f\"'{embedding_file}' file nahi mili. Kripya feature extraction wala cell dobara chalayein.\")\n",
    "    # (Yahan aapko feature extraction wala lamba code daalna hoga agar zaroorat pade)\n",
    "    final_embeddings = None\n",
    "\n",
    "if final_embeddings is not None:\n",
    "    print(\"Final embeddings ka shape:\", final_embeddings.shape)\n",
    "    \n",
    "    # In features ko train aur test mein baant lein\n",
    "    bert_features_train = final_embeddings[:len(train_df)]\n",
    "    bert_features_test = final_embeddings[len(train_df):]\n",
    "\n",
    "    # Ab in naye text features ko image features ke saath jodein\n",
    "    X_train_new_features = np.concatenate([bert_features_train, X_img_train], axis=1)\n",
    "    \n",
    "    print(\"Naye (DistilBERT + Image) features taiyaar hain!\")\n",
    "    print(\"Naye features ka shape:\", X_train_new_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3147139a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Step 1: Text Cleaning ---\n",
      "Text cleaning complete.\n",
      "\n",
      "--- Step 2: Generating and Reducing Features for 8GB RAM ---\n",
      "Generating reduced TF-IDF features (1500)...\n",
      "Loading and reducing BERT features (768 -> 128)...\n",
      "Loading and reducing Image features (2048 -> 256)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nikit\\Desktop\\ML_Challenge\\venv\\Lib\\site-packages\\sklearn\\decomposition\\_pca.py:787: RuntimeWarning: invalid value encountered in divide\n",
      "  self.explained_variance_ratio_ = self.explained_variance_ / total_var\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining all reduced features...\n",
      "All features are ready! Final shape: (75000, 1884)\n",
      "\n",
      "--- Step 3: Training Ensemble Models ---\n",
      "Training LightGBM model...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.487842 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 385577\n",
      "[LightGBM] [Info] Number of data points in the train set: 75000, number of used features: 1628\n",
      "[LightGBM] [Info] Start training from score 2.739217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nikit\\Desktop\\ML_Challenge\\venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training XGBoost model...\n",
      "\n",
      "--- Step 4: Combining predictions ---\n",
      "\n",
      "--- Step 5: Creating Final Submission File ---\n",
      "\n",
      "Aapki final submission file 'test_out_final_attempt.csv' taiyaar hai!\n",
      "   sample_id      price\n",
      "0     100179  15.199087\n",
      "1     245611  14.844592\n",
      "2     146263  22.384439\n",
      "3      95658   9.930439\n",
      "4      36806  30.899461\n"
     ]
    }
   ],
   "source": [
    "# --- Step 0: Sabhi Zaroori Libraries ---\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import gc # Garbage Collector ko import karein\n",
    "\n",
    "# --- Step 1: Text Cleaning ---\n",
    "print(\"--- Step 1: Text Cleaning ---\")\n",
    "# nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "def clean_text(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
    "    words = text.split()\n",
    "    clean_words = [word for word in words if word not in stop_words]\n",
    "    return ' '.join(clean_words)\n",
    "\n",
    "train_df['cleaned_content'] = train_df['catalog_content'].apply(clean_text)\n",
    "test_df['cleaned_content'] = test_df['catalog_content'].apply(clean_text)\n",
    "print(\"Text cleaning complete.\")\n",
    "\n",
    "# --- Step 2: Feature Generation and REDUCTION (Memory Optimized) ---\n",
    "print(\"\\n--- Step 2: Generating and Reducing Features for 8GB RAM ---\")\n",
    "# 2.1 - TF-IDF (Size Kam Kiya Gaya aur Memory Optimized)\n",
    "print(\"Generating reduced TF-IDF features (1500)...\")\n",
    "# --- MEMORY ERROR FIX: dtype=np.float32 add kiya gaya hai ---\n",
    "tfidf = TfidfVectorizer(max_features=1500, dtype=np.float32)\n",
    "text_features_train = tfidf.fit_transform(train_df['cleaned_content'])\n",
    "text_features_test = tfidf.transform(test_df['cleaned_content'])\n",
    "# Memory free karein\n",
    "del train_df['cleaned_content'], test_df['cleaned_content']\n",
    "gc.collect()\n",
    "\n",
    "# 2.2 - BERT Features (PCA se Size Kam Kiya Gaya)\n",
    "print(\"Loading and reducing BERT features (768 -> 128)...\")\n",
    "distilbert_features = np.load('distilbert_embeddings.npy')\n",
    "pca_bert = PCA(n_components=128, random_state=42)\n",
    "bert_features_reduced = pca_bert.fit_transform(distilbert_features)\n",
    "bert_features_train_reduced = bert_features_reduced[:len(train_df)]\n",
    "bert_features_test_reduced = bert_features_reduced[len(train_df):]\n",
    "del distilbert_features, pca_bert\n",
    "gc.collect()\n",
    "\n",
    "# 2.3 - Image Features (PCA se Size Kam Kiya Gaya)\n",
    "print(\"Loading and reducing Image features (2048 -> 256)...\")\n",
    "X_img_train = np.load('X_img_train.npy')\n",
    "X_img_test = np.load('X_img_test.npy')\n",
    "img_features_full = np.vstack([X_img_train, X_img_test])\n",
    "pca_img = PCA(n_components=256, random_state=42)\n",
    "img_features_reduced = pca_img.fit_transform(img_features_full)\n",
    "X_img_train_reduced = img_features_reduced[:len(train_df)]\n",
    "X_img_test_reduced = img_features_reduced[len(train_df):]\n",
    "del X_img_train, X_img_test, img_features_full, pca_img\n",
    "gc.collect()\n",
    "\n",
    "# 2.4 - Sabhi Chhote Features ko Combine Karna\n",
    "print(\"Combining all reduced features...\")\n",
    "X_final_train = hstack([\n",
    "    text_features_train, \n",
    "    csr_matrix(bert_features_train_reduced), \n",
    "    csr_matrix(X_img_train_reduced)\n",
    "]).tocsr()\n",
    "X_final_test = hstack([\n",
    "    text_features_test, \n",
    "    csr_matrix(bert_features_test_reduced), \n",
    "    csr_matrix(X_img_test_reduced)\n",
    "]).tocsr()\n",
    "y_log = np.log1p(train_df['price'])\n",
    "print(\"All features are ready! Final shape:\", X_final_train.shape)\n",
    "del text_features_train, text_features_test, bert_features_train_reduced, bert_features_test_reduced, X_img_train_reduced, X_img_test_reduced\n",
    "gc.collect()\n",
    "\n",
    "# --- Step 3: Ensemble Model Training ---\n",
    "print(\"\\n--- Step 3: Training Ensemble Models ---\")\n",
    "# (Baaki ka code wahi rahega)\n",
    "# 3.1 - LightGBM\n",
    "print(\"Training LightGBM model...\")\n",
    "lgbm_model = lgb.LGBMRegressor(random_state=42, n_estimators=1000, learning_rate=0.05, num_leaves=31)\n",
    "lgbm_model.fit(X_final_train, y_log)\n",
    "lgbm_preds_log = lgbm_model.predict(X_final_test)\n",
    "lgbm_prices = np.expm1(lgbm_preds_log)\n",
    "\n",
    "# 3.2 - XGBoost (Memory-Efficient)\n",
    "print(\"Training XGBoost model...\")\n",
    "xgb_model = xgb.XGBRegressor(random_state=42, n_estimators=1000, learning_rate=0.05, max_depth=7, tree_method='hist', n_jobs=-1)\n",
    "xgb_model.fit(X_final_train, y_log)\n",
    "xgb_preds_log = xgb_model.predict(X_final_test)\n",
    "xgb_prices = np.expm1(xgb_preds_log)\n",
    "\n",
    "# --- Step 4: Combine Predictions ---\n",
    "print(\"\\n--- Step 4: Combining predictions ---\")\n",
    "final_prices = (lgbm_prices * 0.5) + (xgb_prices * 0.5)\n",
    "final_prices[final_prices < 0] = 0\n",
    "\n",
    "# --- Step 5: Final Submission File ---\n",
    "print(\"\\n--- Step 5: Creating Final Submission File ---\")\n",
    "submission_df = pd.DataFrame({'sample_id': test_df['sample_id'], 'price': final_prices})\n",
    "submission_df.to_csv('test_out_final_attempt.csv', index=False)\n",
    "\n",
    "print(\"\\nAapki final submission file 'test_out_final_attempt.csv' taiyaar hai!\")\n",
    "print(submission_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33497479",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading full BERT features to select the best ones...\n",
      "Training a temporary model to find important features...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.693804 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 195840\n",
      "[LightGBM] [Info] Number of data points in the train set: 75000, number of used features: 768\n",
      "[LightGBM] [Info] Start training from score 2.739217\n",
      "\n",
      "Top 256 features have been selected and saved to 'bert_features_selected.npy'.\n",
      "New shape: (150000, 256)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "import pandas as pd\n",
    "\n",
    "# Make sure train_df and y_log are loaded in memory\n",
    "print(\"Loading full BERT features to select the best ones...\")\n",
    "bert_features_train = np.load('distilbert_embeddings.npy')[:len(train_df)]\n",
    "y_log = np.log1p(train_df['price'])\n",
    "\n",
    "print(\"Training a temporary model to find important features...\")\n",
    "# Train a temporary LightGBM model\n",
    "temp_model = lgb.LGBMRegressor(random_state=42)\n",
    "temp_model.fit(bert_features_train, y_log)\n",
    "\n",
    "# Get the feature importances from the model\n",
    "feature_importances = temp_model.feature_importances_\n",
    "\n",
    "# Find the indices of the top 256 most important features\n",
    "top_n = 256\n",
    "top_indices = np.argsort(feature_importances)[::-1][:top_n]\n",
    "\n",
    "# Select only the top features from the full DistilBERT embeddings\n",
    "distilbert_full = np.load('distilbert_embeddings.npy')\n",
    "bert_features_selected = distilbert_full[:, top_indices]\n",
    "\n",
    "# Save this new, smaller feature set to a file\n",
    "np.save('bert_features_selected.npy', bert_features_selected)\n",
    "\n",
    "print(f\"\\nTop {top_n} features have been selected and saved to 'bert_features_selected.npy'.\")\n",
    "print(\"New shape:\", bert_features_selected.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3e2ad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Generating TF-IDF Features ---\n",
      "TF-IDF features created successfully.\n",
      "Shape of text_features_train: (75000, 1500)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "print(\"--- Generating TF-IDF Features ---\")\n",
    "\n",
    "# Make sure the 'cleaned_content' column exists in your dataframes\n",
    "# Use a reduced number of features to save memory\n",
    "tfidf = TfidfVectorizer(max_features=1500, dtype=np.float32)\n",
    "\n",
    "text_features_train = tfidf.fit_transform(train_df['cleaned_content'])\n",
    "text_features_test = tfidf.transform(test_df['cleaned_content'])\n",
    "\n",
    "print(\"TF-IDF features created successfully.\")\n",
    "print(\"Shape of text_features_train:\", text_features_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9799e59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'cleaned_content' column has been created successfully.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Define the cleaning function\n",
    "stop_words = set(stopwords.words('english'))\n",
    "def clean_text(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
    "    words = text.split()\n",
    "    clean_words = [word for word in words if word not in stop_words]\n",
    "    return ' '.join(clean_words)\n",
    "\n",
    "# Apply the function to create the 'cleaned_content' column\n",
    "train_df['cleaned_content'] = train_df['catalog_content'].apply(clean_text)\n",
    "test_df['cleaned_content'] = test_df['catalog_content'].apply(clean_text)\n",
    "\n",
    "print(\"'cleaned_content' column has been created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a54a6f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading full BERT features to select the best ones...\n",
      "Training a temporary model to find important features...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.726773 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 195840\n",
      "[LightGBM] [Info] Number of data points in the train set: 75000, number of used features: 768\n",
      "[LightGBM] [Info] Start training from score 2.739217\n",
      "\n",
      "Top 256 features have been selected and saved to 'bert_features_selected.npy'.\n",
      "New shape: (150000, 256)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "import pandas as pd\n",
    "\n",
    "# Make sure train_df and y_log are loaded in memory\n",
    "print(\"Loading full BERT features to select the best ones...\")\n",
    "bert_features_train = np.load('distilbert_embeddings.npy')[:len(train_df)]\n",
    "y_log = np.log1p(train_df['price'])\n",
    "\n",
    "print(\"Training a temporary model to find important features...\")\n",
    "# Train a temporary LightGBM model\n",
    "temp_model = lgb.LGBMRegressor(random_state=42)\n",
    "temp_model.fit(bert_features_train, y_log)\n",
    "\n",
    "# Get the feature importances from the model\n",
    "feature_importances = temp_model.feature_importances_\n",
    "\n",
    "# Find the indices of the top 256 most important features\n",
    "top_n = 256\n",
    "top_indices = np.argsort(feature_importances)[::-1][:top_n]\n",
    "\n",
    "# Select only the top features from the full DistilBERT embeddings\n",
    "distilbert_full = np.load('distilbert_embeddings.npy')\n",
    "bert_features_selected = distilbert_full[:, top_indices]\n",
    "\n",
    "# Save this new, smaller feature set to a file\n",
    "np.save('bert_features_selected.npy', bert_features_selected)\n",
    "\n",
    "print(f\"\\nTop {top_n} features have been selected and saved to 'bert_features_selected.npy'.\")\n",
    "print(\"New shape:\", bert_features_selected.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1308866c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Loading and Combining all best features ---\n",
      "All best features combined. Final shape: (75000, 2012)\n",
      "\n",
      "--- Starting K-Fold Stacking ---\n",
      "\n",
      "===== FOLD 1 ===== \n",
      "\n",
      "===== FOLD 2 ===== \n",
      "\n",
      "===== FOLD 3 ===== \n",
      "\n",
      "===== FOLD 4 ===== \n",
      "\n",
      "===== FOLD 5 ===== \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import Ridge\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import gc\n",
    "\n",
    "# --- Step 2.1: Sabhi Best Features ko Load aur Combine Karein ---\n",
    "print(\"--- Loading and Combining all best features ---\")\n",
    "# TF-IDF features (1500 wale)\n",
    "# text_features_train, text_features_test\n",
    "\n",
    "# Selected BERT features (256 wale)\n",
    "bert_selected = np.load('bert_features_selected.npy')\n",
    "bert_train_selected = bert_selected[:len(train_df)]\n",
    "bert_test_selected = bert_selected[len(train_df):]\n",
    "\n",
    "# Reduced Image features (256 wale)\n",
    "# X_img_train_reduced, X_img_test_reduced\n",
    "\n",
    "X_final_train = hstack([text_features_train, csr_matrix(bert_train_selected), csr_matrix(X_img_train_reduced)]).tocsr()\n",
    "X_final_test = hstack([text_features_test, csr_matrix(bert_test_selected), csr_matrix(X_img_test_reduced)]).tocsr()\n",
    "y_log = np.log1p(train_df['price'])\n",
    "print(\"All best features combined. Final shape:\", X_final_train.shape)\n",
    "gc.collect()\n",
    "\n",
    "# --- Step 2.2: Stacking Ensemble ---\n",
    "print(\"\\n--- Starting K-Fold Stacking ---\")\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "oof_preds_lgbm = np.zeros(X_final_train.shape[0])\n",
    "oof_preds_xgb = np.zeros(X_final_train.shape[0])\n",
    "test_preds_lgbm = []\n",
    "test_preds_xgb = []\n",
    "\n",
    "lgbm = lgb.LGBMRegressor(random_state=42)\n",
    "xgboost = xgb.XGBRegressor(random_state=42, n_jobs=-1, tree_method='hist')\n",
    "\n",
    "for fold, (train_index, val_index) in enumerate(kf.split(X_final_train, y_log)):\n",
    "    print(f\"\\n===== FOLD {fold+1} ===== \")\n",
    "    # ... (Yahan Stacking ka poora code daalein, jo pehle diya gaya tha) ...\n",
    "    # ... Training LGBM, getting OOF and Test preds ...\n",
    "    # ... Training XGBoost, getting OOF and Test preds ...\n",
    "    # ... Free up memory with gc.collect() at the end of the loop ...\n",
    "\n",
    "# --- Step 2.3: Meta-Model ---\n",
    "# ... (Yahan Meta-Model (Ridge) ko train karne aur final prediction lene ka code daalein) ...\n",
    "# Stacking_prices = ... (Final predictions from the stacking model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3581ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading original image features...\n",
      "Reducing image features using PCA...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nikit\\Desktop\\ML_Challenge\\venv\\Lib\\site-packages\\sklearn\\decomposition\\_pca.py:787: RuntimeWarning: invalid value encountered in divide\n",
      "  self.explained_variance_ratio_ = self.explained_variance_ / total_var\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reduced image features are ready!\n",
      "New shape: (75000, 256)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "\n",
    "# Load your original, large image feature files\n",
    "print(\"Loading original image features...\")\n",
    "X_img_train = np.load('X_img_train.npy')\n",
    "X_img_test = np.load('X_img_test.npy')\n",
    "\n",
    "# Combine them to apply PCA consistently\n",
    "img_features_full = np.vstack([X_img_train, X_img_test])\n",
    "\n",
    "# Reduce the dimensionality using PCA (e.g., from 2048 to 256)\n",
    "print(\"Reducing image features using PCA...\")\n",
    "pca_img = PCA(n_components=256, random_state=42)\n",
    "img_features_reduced = pca_img.fit_transform(img_features_full)\n",
    "\n",
    "# Split the reduced features back into training and testing sets\n",
    "X_img_train_reduced = img_features_reduced[:len(train_df)]\n",
    "X_img_test_reduced = img_features_reduced[len(train_df):]\n",
    "\n",
    "print(\"Reduced image features are ready!\")\n",
    "print(\"New shape:\", X_img_train_reduced.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2a8561e8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_log' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Ek simple baseline (jaise sabhi products ki average price)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m average_price = np.expm1(\u001b[43my_log\u001b[49m).mean()\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Stacking model ke results ko is simple average ke saath thoda sa \"blend\" karein\u001b[39;00m\n\u001b[32m      5\u001b[39m final_blended_prices = stacking_prices * \u001b[32m0.99\u001b[39m + average_price * \u001b[32m0.01\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m: name 'y_log' is not defined"
     ]
    }
   ],
   "source": [
    "# Ek simple baseline (jaise sabhi products ki average price)\n",
    "average_price = np.expm1(y_log).mean()\n",
    "\n",
    "# Stacking model ke results ko is simple average ke saath thoda sa \"blend\" karein\n",
    "final_blended_prices = stacking_prices * 0.99 + average_price * 0.01\n",
    "\n",
    "# Final submission file banayein\n",
    "submission_df = pd.DataFrame({'sample_id': test_df['sample_id'], 'price': final_blended_prices})\n",
    "submission_df.to_csv('test_out_advanced_final.csv', index=False)\n",
    "print(\"Aapki advanced submission file 'test_out_advanced_final.csv' taiyaar hai!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3fc609d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting K-Fold Stacking Ensemble ---\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'X_final_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     12\u001b[39m kf = KFold(n_splits=\u001b[32m5\u001b[39m, shuffle=\u001b[38;5;28;01mTrue\u001b[39;00m, random_state=\u001b[32m42\u001b[39m)\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# To store predictions for the meta-model\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m oof_preds_lgbm = np.zeros(\u001b[43mX_final_train\u001b[49m.shape[\u001b[32m0\u001b[39m])\n\u001b[32m     16\u001b[39m oof_preds_xgb = np.zeros(X_final_train.shape[\u001b[32m0\u001b[39m])\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# To store test predictions from each fold\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'X_final_train' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import Ridge\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import gc\n",
    "\n",
    "# Ensure these variables are in memory before running:\n",
    "# X_final_train, X_final_test, y_log\n",
    "\n",
    "print(\"--- Starting K-Fold Stacking Ensemble ---\")\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# To store predictions for the meta-model\n",
    "oof_preds_lgbm = np.zeros(X_final_train.shape[0])\n",
    "oof_preds_xgb = np.zeros(X_final_train.shape[0])\n",
    "\n",
    "# To store test predictions from each fold\n",
    "test_preds_lgbm = []\n",
    "test_preds_xgb = []\n",
    "\n",
    "# Initialize base models\n",
    "lgbm = lgb.LGBMRegressor(random_state=42)\n",
    "xgboost = xgb.XGBRegressor(random_state=42, n_jobs=-1, tree_method='hist')\n",
    "\n",
    "for fold, (train_index, val_index) in enumerate(kf.split(X_final_train, y_log)):\n",
    "    print(f\"\\n===== FOLD {fold+1} ===== \")\n",
    "    X_train_fold, X_val_fold = X_final_train[train_index], X_final_train[val_index]\n",
    "    y_train_fold, y_val_fold = y_log.iloc[train_index], y_log.iloc[val_index]\n",
    "    \n",
    "    # Train and predict with LightGBM\n",
    "    print(\"Training LightGBM...\")\n",
    "    lgbm.fit(X_train_fold, y_train_fold)\n",
    "    oof_preds_lgbm[val_index] = lgbm.predict(X_val_fold)\n",
    "    test_preds_lgbm.append(lgbm.predict(X_final_test))\n",
    "    \n",
    "    # Train and predict with XGBoost\n",
    "    print(\"Training XGBoost...\")\n",
    "    xgboost.fit(X_train_fold, y_train_fold)\n",
    "    oof_preds_xgb[val_index] = xgboost.predict(X_val_fold)\n",
    "    test_preds_xgb.append(xgboost.predict(X_final_test))\n",
    "    \n",
    "    # Clean up memory\n",
    "    del X_train_fold, X_val_fold, y_train_fold, y_val_fold\n",
    "    gc.collect()\n",
    "\n",
    "print(\"\\n--- Training Meta-Model ---\")\n",
    "# Create training data for the meta-model\n",
    "X_meta_train = np.column_stack([oof_preds_lgbm, oof_preds_xgb])\n",
    "\n",
    "# Train the meta-model\n",
    "meta_model = Ridge()\n",
    "meta_model.fit(X_meta_train, y_log)\n",
    "\n",
    "print(\"\\n--- Making Final Predictions ---\")\n",
    "# Average the test predictions from each fold for the base models\n",
    "final_lgbm_preds = np.mean(test_preds_lgbm, axis=0)\n",
    "final_xgb_preds = np.mean(test_preds_xgb, axis=0)\n",
    "\n",
    "# Create test data for the meta-model\n",
    "X_meta_test = np.column_stack([final_lgbm_preds, final_xgb_preds])\n",
    "\n",
    "# Get final predictions from the meta-model\n",
    "final_preds_log = meta_model.predict(X_meta_test)\n",
    "stacking_prices = np.expm1(final_preds_log)\n",
    "stacking_prices[stacking_prices < 0] = 0\n",
    "\n",
    "print(\"\\nStacking predictions are ready and stored in the 'stacking_prices' variable!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3c89cff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved image features found! Loading from files...\n",
      "Image features loaded successfully.\n",
      "Shape of training image features: (75000, 2048)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# File names for saved features\n",
    "img_train_file = 'X_img_train.npy'\n",
    "img_test_file = 'X_img_test.npy'\n",
    "\n",
    "if os.path.exists(img_train_file) and os.path.exists(img_test_file):\n",
    "    print(\"Saved image features found! Loading from files...\")\n",
    "    X_img_train = np.load(img_train_file)\n",
    "    X_img_test = np.load(img_test_file)\n",
    "    print(\"Image features loaded successfully.\")\n",
    "else:\n",
    "    print(\"Saved image features not found. Starting extraction process (this will take time)...\")\n",
    "    \n",
    "    # Yahan image feature extraction ka poora lamba code daalein\n",
    "    # (jismein 'extract_image_features' function aur ThreadPoolExecutor ka istemal hota hai)\n",
    "    # ...\n",
    "    # ... (Feature extraction code) ...\n",
    "    # ...\n",
    "    \n",
    "    # Process ke ant mein, results ko save karein\n",
    "    np.save(img_train_file, X_img_train)\n",
    "    np.save(img_test_file, X_img_test)\n",
    "    print(\"\\nImage feature extraction complete AND results have been saved for future use!\")\n",
    "\n",
    "print(\"Shape of training image features:\", X_img_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ca913820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'distilbert_embeddings.npy' found! Loading features directly...\n",
      "DistilBERT features loaded successfully.\n",
      "Shape of training text features: (75000, 768)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "embedding_file = 'distilbert_embeddings.npy'\n",
    "\n",
    "if os.path.exists(embedding_file):\n",
    "    print(f\"'{embedding_file}' found! Loading features directly...\")\n",
    "    final_embeddings = np.load(embedding_file)\n",
    "    print(\"DistilBERT features loaded successfully.\")\n",
    "else:\n",
    "    print(f\"'{embedding_file}' not found. Starting extraction process (this will take a long time)...\")\n",
    "    \n",
    "    # Yahan DistilBERT feature extraction ka poora lamba code daalein\n",
    "    # (jismein tokenizer, model, aur batch processing ka istemal hota hai)\n",
    "    # ...\n",
    "    # ... (DistilBERT code) ...\n",
    "    # ...\n",
    "    \n",
    "    np.save(embedding_file, final_embeddings)\n",
    "    print(f\"\\nDistilBERT features extracted AND saved to '{embedding_file}' for future use!\")\n",
    "\n",
    "# Saved ya newly created features ko train/test mein baant lein\n",
    "bert_features_train = final_embeddings[:len(train_df)]\n",
    "bert_features_test = final_embeddings[len(train_df):]\n",
    "print(\"Shape of training text features:\", bert_features_train.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
